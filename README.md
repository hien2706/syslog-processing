```markdown
# Realâ€‘Time Syslog Processing Pipeline

**Data Engineer Project by Tran Dai Hien**  
---

## ğŸš€ Overview

A highly modular, Dockerâ€‘orchestrated realâ€‘time logging pipeline built to collect, process, monitor and store syslog messages at scale. It showcases three processing scenariosâ€”PySpark directâ€‘toâ€‘MongoDB, PySparkâ†’Kafkaâ†’MongoDB, and ksqlDBâ†’Kafkaâ†’MongoDBâ€”with endâ€‘toâ€‘end latency analysis and dashboards.

---

## ğŸ”‘ Key Features

- **Pluggable Processing Scenarios**  
  1. **Syslogâ€‘ng â†’ Kafka â†’ PySpark â†’ MongoDB**  
  2. **Syslogâ€‘ng â†’ Kafka â†’ PySpark â†’ Kafka â†’ MongoDB**  
  3. **Syslogâ€‘ng â†’ Kafka â†’ ksqlDB â†’ Kafka â†’ MongoDB**

- **Realâ€‘Time Monitoring & Alerts**  
  - Prometheus for metrics scraping  
  - Grafana dashboards for latency, throughput and resource usage  

- **Scalable & Containerized**  
  - Docker Compose orchestrates all components  
  - Easily swap connectors, processing engines or storage back ends  

- **Detailed Latency Analysis**  
  - Mean, median, min/max measurements across scenarios  
  - Visualized in Grafana for quick performance comparisons  

---

## ğŸ“‚ Repository Structure

```
.
â”œâ”€â”€ connectors/                    # Kafka Connect plugins
â”‚   â””â”€â”€ mongoâ€‘kafkaâ€‘connectâ€‘1.15.0â€‘confluent.jar
â”œâ”€â”€ demo_logs.txt                 # Sample syslog messages
â”œâ”€â”€ dockerâ€‘compose.yml            # Allâ€‘inâ€‘one orchestrator
â”œâ”€â”€ Dockerfile.PySpark            # PySpark streaming image
â”œâ”€â”€ grafana/                      # Grafana dashboards & provisioning
â”‚   â”œâ”€â”€ dashboards/
â”‚   â”‚   â”œâ”€â”€ message_latency.json
â”‚   â”‚   â””â”€â”€ message_latency_new.json
â”‚   â””â”€â”€ provisioning/
â”‚       â”œâ”€â”€ dashboards/default.yml
â”‚       â””â”€â”€ datasources/prometheus.yml
â”œâ”€â”€ hien_note.txt                 # Personal project notes
â”œâ”€â”€ latencyâ€‘exporter/             # Custom exporter for latency metrics
â”‚   â”œâ”€â”€ Dockerfile.latency_exporter
â”‚   â”œâ”€â”€ latency_exporter.py
â”‚   â””â”€â”€ requirements.txt
â”œâ”€â”€ LICENSE                       # MIT License
â”œâ”€â”€ mongoâ€‘sinkâ€‘connector.json     # Connector configuration
â”œâ”€â”€ prometheus/                   # Prometheus config
â”‚   â””â”€â”€ prometheus.yml
â”œâ”€â”€ PySpark_requirements.txt      # Python dependencies for Spark jobs
â”œâ”€â”€ README.md                     # â† You are here
â”œâ”€â”€ send_demo_logs_rate.sh        # Automated logâ€‘send script
â”œâ”€â”€ spark_job/                    # PySpark streaming applications
â”‚   â”œâ”€â”€ data_processing.py
â”‚   â””â”€â”€ data_processing_kafka.py
â””â”€â”€ syslogâ€‘ng.conf                # Syslogâ€‘ng collector configuration
```

---

## âš™ï¸ Tech Stack

- **Log Collection:** syslogâ€‘ng  
- **Message Broker:** Apache Kafka  
- **Stream Processing:**  
  - PySpark Structured Streaming  
  - ksqlDB (Kafka native SQL engine)  
- **Storage:** MongoDB (via Kafka Connect)  
- **Monitoring:** Prometheus + Grafana  
- **Containerization:** Docker & Docker Compose  
- **Languages & Tools:** PythonÂ 3, Scala (Spark), Bash, JSON, YAML  

---

## ğŸš¦ Getting Started

1. **Clone the repo**  
   ```bash
   git clone https://github.com/yourâ€‘username/syslogâ€‘processing.git
   cd syslogâ€‘processing
   ```

2. **Configure environment**  
   - Verify `syslogâ€‘ng.conf` for your host  
   - Update `mongoâ€‘sinkâ€‘connector.json` (MongoDB URI, database)

3. **Build & launch**  
   ```bash
   dockerâ€‘compose up --build
   ```

4. **Send demo logs**  
   ```bash
   ./send_demo_logs_rate.sh 100  # send 100 messages at default rate
   ```

5. **Explore dashboards**  
   - Grafana: `http://localhost:3000` (user/pass: admin/admin)  
   - Prometheus: `http://localhost:9090`  

---

## ğŸ”„ Pipeline Scenarios & Latency

| Scenario                                                   | Mean Latency | Median | Min   | Max   |
|------------------------------------------------------------|-------------:|-------:|------:|------:|
| 1. syslogâ€‘ng â†’ Kafka â†’ PySpark â†’ MongoDB                   | 1.51Â s       | 1.68Â s | 0.72Â s| 2.27Â s|
| 2. syslogâ€‘ng â†’ Kafka â†’ PySpark â†’ Kafka â†’ MongoDB          | 0.50Â s       | 0.44Â s | 0.21Â s| 0.74Â s|
| 3. syslogâ€‘ng â†’ Kafka â†’ ksqlDB â†’ Kafka â†’ MongoDB           | 0.75Â s       | 0.75Â s | 0.75Â s| 0.88Â s|

**Observations:**  
- ScenarioÂ 2 (buffered via Kafka) achieved the lowest overall latency.  
- ScenarioÂ 3 (ksqlDB) closely trails ScenarioÂ 2 and simplifies deployment by pushing SQL processing into Kafka.  
- ScenarioÂ 1 is direct but impacted by PySpark microâ€‘batch overhead.

---

## ğŸ“„ License

This project is licensed under the **MIT License**. See [LICENSE](LICENSE) for details.

---

## ğŸ“¬ Contact

**Tran Dai Hien**  
ğŸŒ LinkedIn: [yourâ€‘profile]  
âœ‰ï¸ Email: hien2706@tranâ€‘hienâ€‘fedora  

> â€œBuilding scalable, observable data pipelinesâ€”one log at a time.â€  
```
